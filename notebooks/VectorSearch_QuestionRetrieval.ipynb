{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5499b54",
   "metadata": {},
   "source": [
    "\n",
    "# Similar Questions Retrieval\n",
    "\n",
    "This notebook is inspired by the [similar search example of Sentence-Transformers](https://www.sbert.net/examples/applications/semantic-search/README.html#similar-questions-retrieval), and adapted to support [Raft ANN](https://github.com/rapidsai/raft) algorithm.\n",
    "\n",
    "The model was pre-trained on the [Natural Questions dataset](https://ai.google.com/research/NaturalQuestions). It consists of about 100k real Google search queries, together with an annotated passage from Wikipedia that provides the answer. It is an example of an asymmetric search task. As corpus, we use the smaller [Simple English Wikipedia](http://sbert.net/datasets/simplewiki-2020-11-01.jsonl.gz) so that it fits easily into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1a6307",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder, util\n",
    "import time\n",
    "import gzip\n",
    "import os\n",
    "import torch\n",
    "import pylibraft\n",
    "from pylibraft.neighbors import ivf_flat, ivf_pq\n",
    "pylibraft.config.set_output_as(lambda device_ndarray: device_ndarray.copy_to_host())\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "  print(\"Warning: No GPU found. Please add GPU to your notebook\")\n",
    "\n",
    "\n",
    "# We use the Bi-Encoder to encode all passages, so that we can use it with semantic search\n",
    "model_name = 'nq-distilbert-base-v1'\n",
    "bi_encoder = SentenceTransformer(model_name)\n",
    "\n",
    "# As dataset, we use Simple English Wikipedia. Compared to the full English wikipedia, it has only\n",
    "# about 170k articles. We split these articles into paragraphs and encode them with the bi-encoder\n",
    "\n",
    "wikipedia_filepath = 'data/simplewiki-2020-11-01.jsonl.gz'\n",
    "\n",
    "if not os.path.exists(wikipedia_filepath):\n",
    "    util.http_get('http://sbert.net/datasets/simplewiki-2020-11-01.jsonl.gz', wikipedia_filepath)\n",
    "\n",
    "passages = []\n",
    "with gzip.open(wikipedia_filepath, 'rt', encoding='utf8') as fIn:\n",
    "    for line in fIn:\n",
    "        data = json.loads(line.strip())\n",
    "        for paragraph in data['paragraphs']:\n",
    "            # We encode the passages as [title, text]\n",
    "            passages.append([data['title'], paragraph])\n",
    "\n",
    "# If you like, you can also limit the number of passages you want to use\n",
    "print(\"Passages:\", len(passages))\n",
    "\n",
    "# To speed things up, pre-computed embeddings are downloaded.\n",
    "# The provided file encoded the passages with the model 'nq-distilbert-base-v1'\n",
    "if model_name == 'nq-distilbert-base-v1':\n",
    "    embeddings_filepath = 'simplewiki-2020-11-01-nq-distilbert-base-v1.pt'\n",
    "    if not os.path.exists(embeddings_filepath):\n",
    "        util.http_get('http://sbert.net/datasets/simplewiki-2020-11-01-nq-distilbert-base-v1.pt', embeddings_filepath)\n",
    "\n",
    "    corpus_embeddings = torch.load(embeddings_filepath)\n",
    "    corpus_embeddings = corpus_embeddings.float()  # Convert embedding file to float\n",
    "    if torch.cuda.is_available():\n",
    "        corpus_embeddings = corpus_embeddings.to('cuda')\n",
    "else:  # Here, we compute the corpus_embeddings from scratch (which can take a while depending on the GPU)\n",
    "    corpus_embeddings = bi_encoder.encode(passages, convert_to_tensor=True, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4e9b9d",
   "metadata": {},
   "source": [
    "# Vector Search using RAPIDS Raft\n",
    "Now that our embeddings are ready to be indexed and that the model has been loaded, we can use RAPIDS Raft ANN to do our vector search.\n",
    "\n",
    "This is done in two step: First we build the index, then we search it.\n",
    "With pylibraft all you need is those four Python lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad90b4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "params = ivf_pq.IndexParams(n_lists=150, pq_dim=96)\n",
    "pq_index = ivf_pq.build(params, corpus_embeddings)\n",
    "search_params = ivf_pq.SearchParams()\n",
    "\n",
    "def search_raft_pq(query, top_k = 5):\n",
    "    # Encode the query using the bi-encoder and find potentially relevant passages\n",
    "    question_embedding = bi_encoder.encode(query, convert_to_tensor=True)\n",
    "\n",
    "    hits = ivf_pq.search(search_params, pq_index, question_embedding[None], top_k)\n",
    "\n",
    "    # Output of top-k hits\n",
    "    print(\"Input question:\", query)\n",
    "    for k in range(top_k):\n",
    "        print(\"\\t{:.3f}\\t{}\".format(hits[0][0, k], passages[hits[1][0, k]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07935bca",
   "metadata": {},
   "source": [
    "For IVF-PQ we want to reduce the memory footprint while keeping a good accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724dcacb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pq_index_mem = pq_index.pq_dim * pq_index.size * pq_index.pq_bits\n",
    "print(\"IVF-PQ memory footprint: {:.1f} MB\".format(pq_index_mem / 2**20))\n",
    "original_mem = corpus_embeddings.shape[0] * corpus_embeddings.shape[1] * 4\n",
    "print(\"Original dataset: {:.1f} MB\".format(original_mem / 2**20))\n",
    "\n",
    "\n",
    "print(\"Memory saved: {:.1f}%\".format(100 * (1 - pq_index_mem / original_mem)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27d4715",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "search_raft_pq(query=\"Who was Grace Hopper?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc375518",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "search_raft_pq(query=\"Who was Alan Turing?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab154181",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "search_raft_pq(query = \"What is creating tides?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6017ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "params = ivf_flat.IndexParams(n_lists=150)\n",
    "flat_index = ivf_flat.build(params, corpus_embeddings)\n",
    "search_params = ivf_flat.SearchParams()\n",
    "\n",
    "def search_raft_flat(query, top_k = 5):\n",
    "    # Encode the query using the bi-encoder and find potentially relevant passages\n",
    "    question_embedding = bi_encoder.encode(query, convert_to_tensor=True)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    hits = ivf_flat.search(search_params, flat_index, question_embedding[None], top_k)\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Output of top-k hits\n",
    "    print(\"Input question:\", query)\n",
    "    print(\"Results (after {:.3f} seconds):\".format(end_time - start_time))\n",
    "    for k in range(top_k):\n",
    "        print(\"\\t{:.3f}\\t{}\".format(hits[0][0, k], passages[hits[1][0, k]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cfb644",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "search_raft_flat(query=\"Who was Grace Hopper?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5694d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "search_raft_flat(query=\"Who was Alan Turing?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfc3c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "search_raft_flat(query = \"What is creating tides?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
